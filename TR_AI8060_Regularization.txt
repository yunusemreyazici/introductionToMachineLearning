
AI8061 Regularization


Url:https://globalaihub.com/courses/introduction-to-machine-learning/lessons/ai8060-regularization/topic/ai8061-regularization/

Herkese merhaba. Makine öğrenimi yolculuğunuzun bu noktasında, modelimizin yeni verilere ne kadar iyi genelleştirildiğini ve onu iyileştirmek için neler yapabileceğimizi merak ediyor olmalısınız. Bu soruları yanıtlamak için, modelimiz için en yaygın sorunlardan bazılarına bakacağız: yetersiz uyum ve fazla uyum, ayrıca varyans ve yanlılık. Bu sorunların çoğuyla başa çıkmamıza yardımcı olacak bir yaklaşım, düzenlileştirmedir. Birlikte deneyeceğiz. Bilmeniz gereken her şeyi ele alacağız! Araç setinizdeki düzenlileştirme ile daha iyi ve daha doğru modeller yapabileceksiniz. Hadi gidelim!

Overfitting ve underfitting ile başlayacağız. Bir makine öğrenimi modeli, eğitim sırasında kolayca fazla veya eksik takılabilir. Aslında bunlar, modelinizi eğitirken karşılaşabileceğiniz en yaygın sorunlardan bazılarıdır. Peki, ML'de eksik uydurma ve fazla takma tam olarak nedir? Basit bir örnekle bu kavramları anlamaya çalışalım. Bir lise öğrencisini düşünün, Clara. Biyoloji alanında bir üniversite giriş sınavına hazırlanmak zorundadır. Kitabındaki tüm kolay soruları çözüyor. Ancak sınava girmeye gittiğinde daha karmaşık sorular da görecektir. Onları çözemeyecek çünkü sadece kolay olanlar üzerinde eğitim aldı ve daha karmaşık olanları çözmek için pratik yapmadı. Bu yetersiz kalıyor.

Bu nedenle, bir model çok basit olduğunda, eğitim verilerindeki kalıpları bulamadığında ve bu nedenle eğitim setinde ve aynı zamanda görünmeyen verilerde yüksek bir hata ürettiğinde yetersiz uydurmanın meydana geldiğini söyleyebiliriz. Bu modeller aynı zamanda “yüksek düzeyde önyargılı” olarak tanımlanmaktadır. Yanlılık, modelin verilerin karmaşıklığını anlamadaki yetersizliğini ifade eder. Bu grafikte mavi noktalar veriyi, çizgi ise modelimizin tahminidir. Burada tam olarak anlattığımızı görüyoruz: yetersiz uydurma için, verilere tam olarak uymayan çok basit bir düz çizgi görüyoruz. Veri kümesinin büyük bir kısmı yoksayılır. Model iyi performans göstermiyor gibi görünüyor.

Eğitim setinde yeterli veri olmadığında veya karmaşıklıktan yoksun olduğunda, yani örüntüleri tanımak için çok az özelliğe sahip olduğunda genellikle yetersiz kalırız. Clara'yı bir kez daha ele alalım. Şimdi matematik problemleri çözüyor. Sınavda işlenecek tüm farklı matematik soruları için çalışmak yerine, sadece cebir sorularına odaklanıyor. Clara hem kolay hem de zor cebir sorularını çözebilse de sınavın ilerleyen bölümlerinde geometri sorularını gördüğünde çalışmadığı için çözemiyor. Bu fazla uyuyor.

Fazla uydurmayı yetersiz uydurmanın tersi olarak düşünebiliriz. Bu senaryoda model, özel eğitim veri setimiz üzerinde çok fazla eğitilir ve yüksek doğruluk sağlar. Ancak görünmeyen verilere uygulandığında sonucun doğruluğu düşüktür. Bunun nedeni, üzerinde eğitildiği kalıpları araması, ancak test verilerinde genelleme yapamamasıdır. Genelleme, modelin görünmeyen verilere uyum sağlama yeteneğini ifade eder. Bu modeller “yüksek varyanslı modeller” olarak da tanımlanmaktadır. Varyans, bir modelin belirli veri kümelerine duyarlılığını ifade eder. Eğitim verilerinden öğrendiğinden daha fazlasını ezberler. Aşırı uyumun grafiği bize bunun nedenini gösteriyor: Her veri noktasına uyan, ancak genel modeli tanıyamayan ve yeni veriler üzerinde doğru tahminler yapamayan çok karmaşık bir çizgi görüyoruz.

Eğitim verileri çok spesifik olduğunda ve çok fazla özellik içerdiğinde, genellikle aşırı uyum sağlarız. Hem yetersiz uyum hem de fazla uyum kötü tahminlere yol açar. Elde etmek istediğimiz, optimum uyum, iyi bir dengedir. Modelimizin performansı hem varyanstan hem de yanlılıktan etkilenir, bu da yetersiz ve fazla uydurmaya yol açabilir. Varyansı ve yanlılığı ayarlayarak, modelimizi ne çok karmaşık ne de çok basit olmayacak şekilde genelleştirmeyi amaçlıyoruz. Çünkü öğrendiğimiz gibi, yüksek önyargılı fazla uydurma veya yüksek varyanslı yetersiz uydurma, modelimizin doğru tahminler yapması için ideal değildir. Bu arada, yanlılık ve varyans arasında bir değiş tokuş olduğunu belirtmeliyiz. Bu, varyans arttıkça yanlılığın azaldığı ve bunun tersi olduğu anlamına gelir. Grafikte varyans ve yanlılığın buluştuğu nokta, modelimiz için en uygun noktadır.

Artık yüksek sapmanın eksik uydurmaya ve yüksek varyansın fazla uydurmaya yol açtığını öğrendiğimize göre, bu sorunları çözmek için bazı yaklaşımları tartışalım. Başa çıkması daha kolay olduğu için yetersiz uydurma ile başlıyoruz. Yetersiz uyumu çözmeye yönelik genel yaklaşım, verileri daha karmaşık hale getirmektir. Eğitim setindeki gözlem sayısını arttırabiliriz. Tahminleri etkileyebilecek yeni özellikler de ekleyebiliriz. Bu kolaydır, çünkü hiçbir şeyi çıkarmadığımız için eğitim setinden hiçbir orijinal veriyi kaybetmeyiz. Sonunda, modelimiz daha karmaşık hale gelecek ve verilerde gerçek değerlere daha yakın bazı modeller bulmaya çalışacaktır.

Şimdi overfitting'e geçebiliriz. Aşırı uydurma ve yüksek varyansı çözmenin genel fikri, verileri daha az karmaşık hale getirmektir. Verileri daha az karmaşık hale getirmek zordur çünkü karmaşıklıkları ortadan kaldırarak tahminlerde bulunmamıza yardımcı olan yararlı bilgileri kaybedebiliriz. Bu zorluğun üstesinden gelmenin bir yolu, düzenli hale getirmektir. Düzenleme, daha karmaşık kalıpların öğrenilmesini engeller. Bunu, katsayıları sıfıra doğru küçülterek yapar, böylece daha az önemli özelliklerin etkisi azaltılır ve yüksek varyans önlenir. Düzenleme, L1 ve L2 olarak adlandırılan kayıp fonksiyonlarını kullanır. En basit ve en yaygın kayıp fonksiyonlarından biri olan “Mean Squared Error” MSE'ye zaten aşinasınız. L1 ve L2 kayıp fonksiyonlarını bunun değiştirilmiş bir versiyonu olarak düşünebilirsiniz.

Gördüğünüz gibi, formülün ilk kısmı basitçe bizim MSE fonksiyonumuz. Ama ona eklenen bir parametre daha var. Bu parametre, katsayı büyüklüğünün mutlak değerine eşittir; sadece katsayıların boyutunu kısıtlar veya cezalandırır. Özellikleri kaldırmak için kement regresyon, daha az tahmin özelliği olan özelliklerin katsayısını sıfıra indirir. Bunun için, modelimizin esnekliğini ne kadar cezalandırmak istediğimizi belirtmek için kullanılan λ(lambda) ayarlama parametresine sahibiz. Daha az öngörülü derken, saç rengini birinin IQ'sunu tahmin etmek için bir özellik olarak kullanmayı düşünün. Burada saç rengi daha az tahmin edici olan özelliktir ve bu nedenle katsayı sıfıra çekilerek ortadan kaldırılmaktadır.

Ridge regresyonunda kullanılan diğer L2 kayıp fonksiyonuna geçelim. L2 kayıp fonksiyonu, L1'e çok benzer, tek fark, katsayıların büyüklüğünün karesinin alınmış olmasıdır. Özellikleri kaldırmak için, sırt regresyonu tüm katsayıları eşit olarak küçültür, ancak hiçbirini sıfıra indirmez. Bu durumda, özelliğin daha tahmin edici veya daha az tahmin edici olması bizim için önemli değildir, genel amaç, tüm katsayıları küçülterek genel karmaşıklığı azaltmaktır. Peki, ne zaman L1'i ve ne zaman L2'yi kullanıyoruz?

Genel olarak, L2 kayıp fonksiyonu daha yaygındır. Ancak veri setinde aykırı değerler olduğunda L2 kayıp fonksiyonunu kullanmak, gerçek ve tahmin edilen değerler arasındaki farkların karelerini almak çok daha büyük bir hataya yol açacağından kullanışlı değildir. Bu durumda aykırı değerlerden etkilenmediği için L1 Kayıp Fonksiyonunu kullanmak daha doğru olacaktır. 1 ve 100 sayılarını ele alalım. Farkı L1 kayıp fonksiyonunu kullanarak hesaplarsak hata 99 olur ama L2 kayıp fonksiyonu 99'un karesi ile hesaplarsak 9801 olur. Aykırı değerleri de kaldırabiliriz. ve ardından L2 Kayıp İşlevini kullanın.

Şimdi bu kavramların pratikte nasıl işlediğini görelim. Bu pratik örnekte, L2 regresyonunu kullanarak tahminler yapmaya çalışacağız. Ve sonra, MSE metriğini kullanarak doğruluk üzerindeki düzenlileştirme sonuçlarını göstereceğiz. Gerekli kitaplıkları içe aktarmakla başlayalım. Şimdi, örnek veri setini içe aktarıyoruz. Ardından veri setimizi özelliklere ve hedeflere ayırıyoruz. Son olarak, onları tren ve test veri kümelerine ayırdık. Sırt regresyonunu kullanalım ve sonucu lineer regresyonla karşılaştıralım! Kolayca kullanabilmek için iki modelin her birini bir değişkene atadık. Bazı hiperparametreler kullandığımızı fark etmişsinizdir, bunlardan biri lambdamızdır ve daha önce açıklandığı gibi ayarlama parametremizdir. "Normalize" parametresi, tüm veri noktalarını 0 ile 1 aralığına dönüştürür ve bu da veri kümemizdeki çeşitliliği azaltır. Farklı hiperparametreler ayarlayarak, modelimizi geliştirebiliriz. Ardından, eğitim veri setini kullanarak her iki modelimizi de eğitebiliriz. Son olarak, test veri setini kullanarak tahminler yapmaya hazırız.

Şimdi iki modeli nasıl karşılaştırabiliriz? Evet, metrikleri kullanabiliriz! Ortalama hata karesi MSE'yi hesaplayarak, lineer ve ridge regresyon performansları arasında bir karşılaştırma yapabileceğiz. Farkı görmek için sonuçları yazdıralım. Sırt düzenlemesi için MSE'nin daha düşük olduğunu fark edebilirsiniz, bu da tahminlerinin daha iyi olduğu anlamına gelir. Sonuçlarda sadece küçük bir iyileşme görmemize rağmen, boyutları daha büyük ve daha karmaşık oldukları için gerçek hayattaki veri kümeleri için aynı olmayacak. Veri çeşitliliği daha fazladır ve bu nedenle üst üste takmak daha kolaydır. Bu gibi durumlarda düzenlileştirme kullanmak, doğruluğu üzerinde daha büyük bir etkiye sahip olacaktır.

Bu videoda fazla uydurma, yetersiz uydurma, yanlılık ve varyans terimlerini içeren düzenlileştirme hakkında bilgi edindiniz. Normalleştirmenin, modelimizin gereğinden az ve fazla takılmasını önlemek ve yüksek yanlılığı ve yüksek varyansı azaltmak için nasıl düzenli hale getirmeye yardımcı olabileceğini öğrendik. Son olarak, bir düzenlileştirme yöntemini bir uygulamada nasıl uygulayacağınızı göstermek için pratik bir alıştırma yaptık. Bir sonraki oturumda görüşmek üzere!