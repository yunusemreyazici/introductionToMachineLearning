Yine buradayız! Şimdiye kadar, makine öğreniminin çözebileceği temel sorunları öğrendiniz. Sınıflandırma, regresyon ve kümeleme. Modelinizi iyileştirmek ve fazla uydurmayı önlemek için düzenli hale getirme yöntemlerini de ele aldık. Bundan sonra yaygın olarak kullanılan makine öğrenimi algoritmalarına ve bunların arkasındaki mantığa odaklanacağız. Temel olarak sınıflandırma problemlerini çözmek için kullanılan en basit algoritmalardan biri olan ve birçok geliştiricinin kıyaslama olarak kullandığı algoritma olan Naive Bayes ile başlayacağız. Başlayalım!

Televizyondaki hava durumu tahminlerinin neden neredeyse her zaman doğru olduğunu hiç merak ettiniz mi? Hayır, onlar falcı değiller. Sadece “olasılık” kullanırlar. Bu aynı zamanda Naive Bayes sınıflandırıcı algoritmasının arkasındaki sırdır. Metin analizinden spam tespitine ve çok daha fazlasına kadar pek çok soruna basit ama güçlü çözümler bulmamıza yardımcı olabilir. Bu algoritmaya geçmeden önce, olasılığı gözden geçirmek iyi bir fikirdir. Bilmeniz gereken olasılık teorisinde yaygın olarak kullanılan bazı notasyonlar vardır. Bir tazeleme ile başlayalım.

Olasılık, bir olayın olma olasılığını açıklayan matematiğin dalıdır. 0 ile 1 arasındaki bir değer, bir olayın olma olasılığını temsil eder. 1'e yakınsa olma olasılığı daha yüksektir ve 0'a yakınsa daha az olasıdır. Örnek olarak adil bir madeni parayı ele alalım, bu da her iki tarafa, tura veya tura gelme şansının eşit olduğu anlamına gelir. . Yani, madeni paranın tura veya tura gelme olasılığı 0,5'tir. Küçük örneklem boyutlarının olasılığı doğru şekilde yansıtmayabileceğini unutmayın. Madeni parayı sadece dört kez attığınızı düşünün, 3 kez tura, yalnızca bir kez yazı gelebilir. Tura gelme olasılığının 0,75 olduğu sonucuna varmamalıyız. Bu nedenle küçük örneklem büyüklüğüne göre bir olayın olasılığı hakkında varsayımlarda bulunamayız.

Sınıflandırmanın temel amacı, verilen bir özellik setine karşılık gelen sınıfı bulmaktır. Naive Bayes sınıflandırıcı algoritması, koşullu olasılığa odaklanan “Bayes teoremi” adlı ünlü bir teoreme dayanmaktadır. Koşullu olasılık, başka bir "B" olayının zaten gerçekleşmiş olması koşuluyla, bir "A" olayının olma olasılığıdır. Örneğin, 'A' olayını "ateşi var" ve "B" olayını "Covid-19 ile enfekte" olarak kabul edin. Koşullu olasılıkla şu soruyu sorabiliriz: Covid-19 ile enfekte olduğunuza göre ateşinizin olma olasılığı nedir?

Bayes teoremi, koşullu olasılığın bir uzantısıdır. Bir anlamda, ters akıl yürütmeyi kullanmamıza izin verir. Covid örneğimizde bu, enfekte olduğunuza göre ateşinizin olma olasılığını zaten bildiğimiz anlamına gelir. Ama aslında ateşiniz varsa Covid olma olasılığıyla ilgileniyoruz. Sezgisel olarak olasılığın aynı olduğunu varsayabilirsiniz. Ancak burada bağımlı olaylar olduğu için, her iki olayın da kendi başlarına olma olasılıklarını da göz önünde bulundurmamız gerekir. Bunun neye benzediğini anlamak için Bayes teoreminin formülüne bakalım. Yalnızca 'A'nın meydana gelme olasılığı, önceki olasılık olarak anılır, P(A). Hesaplamak istediğimiz olasılık, 'B' olayının meydana gelmesi koşuluyla, 'A' olayının olasılığıdır. Buna arka olasılık, AgivenB denir. A olayının gerçekleşmiş olması koşuluyla, B olayının olma olasılığı BverilenA'dır. Sadece B'nin olasılığı P(B)'dir.

Artık Bayes teoreminin neyle ilgili olduğunu anladığımıza göre, bu güçlü kavramın makine öğrenimi problemlerine nasıl uygulandığına bakabiliriz. Bayes teoreminin akıl yürütmeyi tersine çevirmemize nasıl izin verdiğini nasıl açıkladığımızı hatırlıyor musunuz? Naive Bayes algoritması aynı şeyi sınıf ve özellikleri için yapar. Bir özelliğin bir sınıfa ait olma olasılığını hesaplamak yerine konuya başka bir açıdan yaklaşır. Burada bir özelliğimiz var ve bunun belirli bir sınıfa ait olma olasılığını hesaplamak istiyoruz. Daha iyi anlamak için bilimkurgu ve tarih olmak üzere iki sınıfa ait 20 makalemiz olduğunu varsayalım. Görüyorsunuz, özellikler olarak 5 kelimemiz var (bilim, zaman, devrim, evrim ve hayal). Bu kelimelerin hem bilim kurgu makalelerinde hem de tarih makalelerinde kaç kez geçtiğini sayıyoruz. Daha sonra bilimkurgu makalelerinde bulunduğuna göre her kelimenin olasılığını hesaplıyoruz ve tarih makalelerinde her kelime için aynı işlemi yapıyoruz. Örneğin bir tarih yazısında “devrim” kelimesinin görülme olasılığı 6/21 = 0,285'tir.

Diyelim ki bize yeni bir dizi kelime verildi: “Bilimin Evrimini Hayal Edin”. Şimdi hangi makaleye ait olma olasılığının daha yüksek olduğunu tahmin etmek istiyoruz. İlk olarak, bilim kurgu sınıfına ait olduğunu varsayarak her kelimenin koşullu olasılığını hesaplıyoruz. Sonra tarih dersine ait olduğu için aynısını yapıyoruz. Bu senaryoda tarih dersinden ziyade bilim kurgu dersine ait olduğunu tahmin ederken koşullu olasılığın daha yüksek olduğunu görebiliriz. Naive Bayes algoritmasının genel konsepti budur. Bayes teoreminin kullanımını anladığımıza göre, neden naif denildiği hakkında konuşalım.

Model, bir sınıfa verilen bir özelliğin koşullu olasılığını hesaplarken, diğer herhangi bir özelliğin etkisini hesaba katmaz. Özelliklerin birbirinden bağımsız olduğunu varsayar. Bir sınıfa ait olduğu göz önüne alındığında, bize her bir özelliğin olasılığını, açıklama esnekliğini verir. Örneğimizde bu, kelimelerimizin sırası “Evolution of Imagine Science” olarak değiştiğinde, bize aynı olasılığı vereceği anlamına gelir. Bu algoritmanın saflığı bazı sorunlara ve sınırlamalara neden olabilir. Çoğu gerçek dünya durumunda, bazı özelliklerimiz muhtemelen birbirine bağımlı olduğundan, yanlış sonuçlara varabiliriz. Ancak bazı avantajlar da var! Çok hızlı ve basit. Gerçek bir zaman kazandırıcı. Naive Bayes, genellikle zor olan az miktarda eğitim verisiyle de iyi sonuçlar üretebilir. Naive Bayes algoritmasının çoğunlukla bir modelin kıyaslaması için kullanılmasının nedeni de budur. Ve son olarak, bu algoritma ayrıklaştırarak sürekli verilerle iyi çalışır, bu da her değeri bir aralıkla değiştirmek ve onu ayrık veri olarak kabul etmek anlamına gelir.

Şimdi pratik bir örnekle devam edelim. Sklearn'de üç tür Naive Bayes Sınıflandırıcısı vardır; Bernoulli Naive Bayes, Multinomial Naive Bayes ve Gaussian Naive Bayes. Verilerimiz doğru veya yanlış, evet veya hayır gibi ikili olduğunda Bernoulli Naive Bayes kullanırız. Aile üyelerinin sayısı veya bir kitaptaki sayfalar gibi ayrı değerlere sahip olduğumuzda Multinomial Naive Bayes kullanırız. Tüm özelliklerimiz sıcaklık veya yükseklik gibi sürekli değişkenler olduğunda Gauss Naive Bayes kullanırız. Sadece sürekli değişkenleri olan sınıflandırma oturumumuzdan tümörler hakkındaki veri setini alalım. Gauss Naive Bayes algoritmasını kullanıyoruz. Her zaman olduğu gibi, veri dosyamızı okumak için Pandas kitaplığını içe aktararak başlıyoruz. Ardından veri kümesinin ilk birkaç satırını okuyup görüntüleyebiliriz.

Sınıflandırma dersinde yaptığımız gibi, etiket kodlamayı kullanarak hedef değişkenimizi kategorik tipten sayısal tipe dönüştürmemiz gerekiyor. Ardından, özelliklerimizi ve bir hedefi tanımlamaya devam edebiliriz. Özellikleri ve hedefi tanımladıktan sonra, bunları eğitme ve test etme olarak ayırabiliriz. Şimdi, en heyecanlı kısım! Gaussian Naive Bayes adlı modelimizi oluşturalım, ona eğitim verileriyle bazı gizli kalıplar öğretelim ve son olarak tahminlerde bulunmak için kullanalım. Artık tahminlerimizin gücünü kontrol edebiliriz.

Sonuçlara bakıldığında %90'ın üzerinde doğruluk ve kesinlik elde ettik ve bu iyi bir sonuç olarak değerlendirilebilir. Hatırlarsınız sınıflandırma dersimizde aynı veri seti üzerinde lojistik regresyon kullanırken 0.97(doksan yedi) doğruluk elde etmiştik. . Bunu Naive Bayes sonucumuzla karşılaştırdığımızda yine çok iyi bir doğruluk elde ettiğimizi görebiliriz ancak lojistik regresyon ile elde ettiğimiz sonuçtan daha azdır. Bu da yine Naive Bayes algoritmasının naif yönünü göstermekte ve özelliklerin birbirine bağımlılığı göz önüne alındığında. Harika, Gaussian Naive Bayes algoritmasını pratikte başarılı bir şekilde kullandık, Bernoulli Naive Bayes ve Multinomial Naive Bayes'i de farklı veri kümelerinde tamamen aynı şekilde kullanabilirsiniz.

Bu videoda, bazı olasılıksal kavramlar ve notasyonlar hakkındaki bilgilerinizi tazeledik. Ve bu bilgiyi kullanarak Naive Bayes algoritmasının altında yatan mantığı ve saflığının makine öğrenimi problemlerini basit ve hızlı bir şekilde çözmemize nasıl yardımcı olduğunu öğrendik. Bir sonraki heyecan verici makine öğrenimi modelinde görüşmek dileğiyle!

